{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f542eba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import selenium\n",
    "import html5lib\n",
    "import html.parser\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from selenium.common.exceptions import NoSuchElementException \n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import requests\n",
    "import re \n",
    "import urllib\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5176bb00",
   "metadata": {},
   "source": [
    "## Q1. Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea5b4ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8384fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing url:\n",
    "url=' https://www.amazon.in/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cef9eb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter any nameGuitar\n"
     ]
    }
   ],
   "source": [
    "search_input = driver.find_element_by_id('twotabsearchtextbox')\n",
    "search_input.send_keys (input('Enter any name'))\n",
    "#submit button\n",
    "submit_btn_search = driver.find_element_by_id('nav-search-submit-button')\n",
    "submit_btn_search.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e264cbf4",
   "metadata": {},
   "source": [
    "## 2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b91dc3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = []\n",
    "\n",
    "for i in driver.find_elements_by_xpath('//a[@class=\"a-link-normal s-no-outline\"]'):\n",
    "    urls.append(i.get_attribute('href'))\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb6071b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#page 1 variables\n",
    "\n",
    "Brand_Name1 = []\n",
    "NameOfTheProduct_1= []\n",
    "Rating_1= []\n",
    "No_ofRatings_1= []\n",
    "Price1 = []\n",
    "Return_Exchange1 = []\n",
    "ExpectedDelivery1 = []\n",
    "Availability_1= []\n",
    "OtherDetails1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "360345b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using try and except methord:\n",
    "\n",
    "# scraping the product brand   \n",
    "\n",
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    #fetching brand name data\n",
    "    try:\n",
    "        names1 = driver.find_element_by_xpath('//*[@id=\"productOverview_feature_div\"]/div/table/tbody/tr[1]/td[2]')\n",
    "        Brand_Name1.append(names1.text.replace('\\n', 'new line'))\n",
    "    except:\n",
    "        Brand_Name1.append('-')\n",
    "        \n",
    "    #fetching Name Of The Product\n",
    "    try:\n",
    "        nameproduct1 = driver.find_element_by_xpath('//*[@id=\"productTitle\"]')\n",
    "        NameOfTheProduct_1.append(nameproduct1.text.replace('\\n', 'new line'))\n",
    "    except:\n",
    "        NameOfTheProduct_1.append('-')\n",
    "        \n",
    "    # fetching rating\n",
    "    try:\n",
    "        Rating1 = driver.find_element_by_xpath('//*[@id=\"a-popover-content-7\"]/div/div/div/div[1]/span')\n",
    "        Rating_1.append(Rating1.text.replace('\\n', 'new line'))\n",
    "    except:\n",
    "        Rating_1.append('-')\n",
    "        \n",
    "    # fetching No of Rating\n",
    "    try:\n",
    "        NoofRatings1 = driver.find_element_by_xpath('//*[@id=\"acrCustomerReviewText\"]')\n",
    "        No_ofRatings_1.append(NoofRatings1.text.replace('\\n', 'new line'))\n",
    "    except:\n",
    "        No_ofRatings_1.append('-')\n",
    "        \n",
    "    # fetching Price\n",
    "    try:\n",
    "        Prices1 = driver.find_element_by_xpath('//*[@id=\"priceblock_ourprice\"]')\n",
    "        Price1.append(Prices1.text.replace('\\n', 'new line'))\n",
    "    except:\n",
    "        Price1.append('-')\n",
    "        \n",
    "    # fetching Return_Exchange1\n",
    "    try:\n",
    "        Exchange1 = driver.find_element_by_xpath('//*[@id=\"RETURNS_POLICY\"]/span/div[2]/a')\n",
    "        Return_Exchange1.append(Exchange1.text.replace('\\n','new line'))\n",
    "    except:\n",
    "        Return_Exchange1.append('-')\n",
    "        \n",
    "    # fetching Expected Delivery\n",
    "    try:\n",
    "        ExpectedDeliverys1 = driver.find_element_by_xpath('/html/body/div[2]/div[2]/div[5]/div[3]/div[4]/div[12]/div/div/b')\n",
    "        ExpectedDelivery1.append(ExpectedDeliverys1.replace('\\n','new line'))\n",
    "    except:\n",
    "        ExpectedDelivery1.append('-')\n",
    "        \n",
    "    #fetching Availability_1\n",
    "    try: \n",
    "        Availabilitys_1 = driver.find_element_by_xpath('/html/body/div[2]/div[2]/div[5]/div[20]/div/div/div/div[2]/div[1]/div/div/table/tbody/tr[4]/td')\n",
    "        Availability_1.append(Availabilitys_1.replace('\\n','new line'))\n",
    "    except:\n",
    "        Availability_1.append('-')\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7270e1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 64, 64, 64, 64, 64, 64, 64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking data size's of all collected data\n",
    "len(Brand_Name1),len(NameOfTheProduct_1),len(Rating_1),len(No_ofRatings_1),len(Price1),len(Return_Exchange1),len(ExpectedDelivery1),len(Availability_1),len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a0e89b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating data frame\n",
    "df = pd.DataFrame()\n",
    "df['Brand Name']=Brand_Name1\n",
    "df['Name of the Product']=NameOfTheProduct_1\n",
    "df['Rating']=Rating_1\n",
    "df['No of Ratings']=No_ofRatings_1\n",
    "df['Price']=Price1\n",
    "df['Return Exchange']=Return_Exchange1\n",
    "df['Expected Delivery']=ExpectedDelivery1\n",
    "df['Availability']=Availability_1\n",
    "df['Product Url']=urls\n",
    "df.to_csv('file1.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db05dcb5",
   "metadata": {},
   "source": [
    "### 3. Write a python program to access the search bar and search button on images.google.com and scrape 10images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3a4d256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google_images_download in c:\\users\\rahul\\anaconda3\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: selenium in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from google_images_download) (4.1.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from selenium->google_images_download) (0.9.2)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from selenium->google_images_download) (1.26.4)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from selenium->google_images_download) (0.20.0)\n",
      "Requirement already satisfied: idna in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->google_images_download) (2.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->google_images_download) (1.2.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->google_images_download) (1.10)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->google_images_download) (1.14.5)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->google_images_download) (2.3.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->google_images_download) (20.3.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->google_images_download) (1.1.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium->google_images_download) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium->google_images_download) (1.1.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium->google_images_download) (3.4.7)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium->google_images_download) (20.0.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium->google_images_download) (2020.12.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium->google_images_download) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium->google_images_download) (1.15.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\rahul\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->google_images_download) (0.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google_images_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "630b0b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing google_images_download module\n",
    "from google_images_download import google_images_download \n",
    "  \n",
    "# creating object\n",
    "response = google_images_download.googleimagesdownload() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ea62d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Providing search items:\n",
    "search1 = 'Fruits,'\n",
    "search2 = 'Cars,'\n",
    "search3 = 'Machine Learning,'\n",
    "search4 = 'Guitar,'\n",
    "search5 = 'Cakes,'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80d86687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruits\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome()\n",
    "url = 'http://www.images.google.com'  \n",
    "driver.get(url)\n",
    "search_bar = driver.find_element_by_xpath('//input[@class=\"gLFyf gsfi\"]')    \n",
    "search_bar.send_keys(input())                                                \n",
    "search_btn = driver.find_element_by_xpath('//span[@class=\"z1asCe MZy1Rb\"]')  \n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8b86a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 0 of 10 images\n",
      "Downloading 1 of 10 images\n",
      "Downloading 2 of 10 images\n",
      "Downloading 3 of 10 images\n",
      "Downloading 4 of 10 images\n",
      "Downloading 5 of 10 images\n",
      "Downloading 6 of 10 images\n",
      "Downloading 7 of 10 images\n",
      "Downloading 8 of 10 images\n",
      "Downloading 9 of 10 images\n",
      "Downloading 10 of 10 images\n"
     ]
    }
   ],
   "source": [
    "for i in range(2000):\n",
    "    driver.execute_script(\"window.scrollBy(0,10000)\")\n",
    "    images = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "img_urls = []\n",
    "for image in images:\n",
    "    source = image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "len(img_urls)      \n",
    "for i in range(len(img_urls)):\n",
    "    if i>10:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\".format(i,10))\n",
    "    response = requests.get(img_urls[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69ce8496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cars\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.images.google.com' \n",
    "driver.get(url)\n",
    "search_bar = driver.find_element_by_xpath('//input[@class=\"gLFyf gsfi\"]')\n",
    "search_bar.send_keys(input())\n",
    "search_btn = driver.find_element_by_xpath('//span[@class=\"z1asCe MZy1Rb\"]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "074b03d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 0 of 10 images\n",
      "Downloading 1 of 10 images\n",
      "Downloading 2 of 10 images\n",
      "Downloading 3 of 10 images\n",
      "Downloading 4 of 10 images\n",
      "Downloading 5 of 10 images\n",
      "Downloading 6 of 10 images\n",
      "Downloading 7 of 10 images\n",
      "Downloading 8 of 10 images\n",
      "Downloading 9 of 10 images\n",
      "Downloading 10 of 10 images\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "    images = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "img_urls = []\n",
    "for image in images:\n",
    "    source = image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "len(img_urls)      \n",
    "for i in range(len(img_urls)):\n",
    "    if i>10:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\".format(i,10))\n",
    "    response = requests.get(img_urls[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99243603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.images.google.com' \n",
    "driver.get(url)\n",
    "search_bar = driver.find_element_by_xpath('//input[@class=\"gLFyf gsfi\"]')\n",
    "search_bar.send_keys(input())\n",
    "search_btn = driver.find_element_by_xpath('//span[@class=\"z1asCe MZy1Rb\"]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a10f237f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 0 of 10 images\n",
      "Downloading 1 of 10 images\n",
      "Downloading 2 of 10 images\n",
      "Downloading 3 of 10 images\n",
      "Downloading 4 of 10 images\n",
      "Downloading 5 of 10 images\n",
      "Downloading 6 of 10 images\n",
      "Downloading 7 of 10 images\n",
      "Downloading 8 of 10 images\n",
      "Downloading 9 of 10 images\n",
      "Downloading 10 of 10 images\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "    images = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "img_urls = []\n",
    "for image in images:\n",
    "    source = image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "len(img_urls)      \n",
    "for i in range(len(img_urls)):\n",
    "    if i>10:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\".format(i,10))\n",
    "    response = requests.get(img_urls[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28783804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guitar\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.images.google.com' \n",
    "driver.get(url)\n",
    "search_bar = driver.find_element_by_xpath('//input[@class=\"gLFyf gsfi\"]')\n",
    "search_bar.send_keys(input())\n",
    "search_btn = driver.find_element_by_xpath('//span[@class=\"z1asCe MZy1Rb\"]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34895d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 0 of 10 images\n",
      "Downloading 1 of 10 images\n",
      "Downloading 2 of 10 images\n",
      "Downloading 3 of 10 images\n",
      "Downloading 4 of 10 images\n",
      "Downloading 5 of 10 images\n",
      "Downloading 6 of 10 images\n",
      "Downloading 7 of 10 images\n",
      "Downloading 8 of 10 images\n",
      "Downloading 9 of 10 images\n",
      "Downloading 10 of 10 images\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "    images = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "img_urls = []\n",
    "for image in images:\n",
    "    source = image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "len(img_urls)      \n",
    "for i in range(len(img_urls)):\n",
    "    if i>10:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\".format(i,10))\n",
    "    response = requests.get(img_urls[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fb9897f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cakes\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.images.google.com' \n",
    "driver.get(url)\n",
    "search_bar = driver.find_element_by_xpath('//input[@class=\"gLFyf gsfi\"]')\n",
    "search_bar.send_keys(input())\n",
    "search_btn = driver.find_element_by_xpath('//span[@class=\"z1asCe MZy1Rb\"]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a18ee528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 0 of 10 images\n",
      "Downloading 1 of 10 images\n",
      "Downloading 2 of 10 images\n",
      "Downloading 3 of 10 images\n",
      "Downloading 4 of 10 images\n",
      "Downloading 5 of 10 images\n",
      "Downloading 6 of 10 images\n",
      "Downloading 7 of 10 images\n",
      "Downloading 8 of 10 images\n",
      "Downloading 9 of 10 images\n",
      "Downloading 10 of 10 images\n"
     ]
    }
   ],
   "source": [
    "for i in range(2000):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "    images = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "img_urls = []\n",
    "for image in images:\n",
    "    source = image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "len(img_urls)      \n",
    "for i in range(len(img_urls)):\n",
    "    if i>10:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\".format(i,10))\n",
    "    response = requests.get(img_urls[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c516c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935888b0",
   "metadata": {},
   "source": [
    "## 4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) onwww.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12e8920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an instance of webdriver for google chrome\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "521b9132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using webdriver we'll now open the flipkart website in chrome\n",
    "url = 'https://flipkart.com'\n",
    "\n",
    "page = driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56bfcb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile=driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input')\n",
    "mobile.send_keys(\"mobile phones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75f02fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search bar:\n",
    "search_bar = driver.find_element_by_class_name('_34RNph')\n",
    "search_bar.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5996e013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(search_item):\n",
    "    '''\n",
    "    This function fetches the URL of the item that you want to search\n",
    "    '''\n",
    "    template = 'https://www.flipkart.com/search?q={}&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_4_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_4_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=mobile+phones&requestId=e625b409-ca2a-456a-b53c-0fdb7618b658&as-backfill=on'\n",
    "    # We'are replacing every space with '+' to adhere with the pattern \n",
    "    search_item = search_item.replace(\" \",\"+\")\n",
    "    return template.format(search_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65f0087f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.flipkart.com/search?q=mobile+phones&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_4_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_4_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=mobile+phones&requestId=e625b409-ca2a-456a-b53c-0fdb7618b658&as-backfill=on\n"
     ]
    }
   ],
   "source": [
    "# Checking whether the function is working properly or not\n",
    "url = get_url('mobile phones')\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4663d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58803e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = soup.find_all('a',{'class':\"_1fQZEK\"})\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f25e6efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# picking the 1st card from the complete list of cards\n",
    "item = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4581170a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MOTOROLA G60 (Moonless, 128 GB)'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the model of the phone from the 1st card\n",
    "model = item.find('div',{'class':\"_4rR01T\"}).text\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e44895a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.2'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting Stars from 1st card\n",
    "star = item.find('div',{'class':\"_3LWZlK\"}).text\n",
    "star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12eb15a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'86,340 Ratings'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting Number of Ratings from 1st card\n",
    "num_ratings = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[0:item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')].strip()\n",
    "num_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8372c40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8,813 Reviews'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting Number of Reviews from 1st card\n",
    "reviews = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')+1:].strip()\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d7b8039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6 GB RAM '"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting RAM from the 1st card\n",
    "ram = item.find('li',{'class':\"rgWa7D\"}).text[0:item.find('li',{'class':\"rgWa7D\"}).text.find('|')]\n",
    "ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "02d2a2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'128 GB RO'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting Storage/ROM from 1st card\n",
    "storage = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][0:10].strip()\n",
    "storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be85a293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting whether there is an option of expanding the storage or not\n",
    "expandable = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][13:]\n",
    "expandable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8e1af2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'17.22 cm (6.78 inch) Full HD+ Display'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the display option from the 1st card\n",
    "display = item.find_all('li')[1].text.strip()\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84c8e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_phone_model_info(item):\n",
    "    \"\"\"\n",
    "    This function extracts model, price, ram, storage, stars , number of ratings, number of reviews, \n",
    "    storage expandable option, display option, camera quality, battery , processor, warranty of a phone model at flipkart\n",
    "    \"\"\"\n",
    "    # Extracting the model of the phone from the 1st card\n",
    "    model = item.find('div',{'class':\"_4rR01T\"}).text\n",
    "    # Extracting Stars from 1st card\n",
    "    star = item.find('div',{'class':\"_3LWZlK\"}).text\n",
    "    # Extracting Number of Ratings from 1st card\n",
    "    num_ratings = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[0:item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')].strip()\n",
    "    # Extracting Number of Reviews from 1st card\n",
    "    reviews = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')+1:].strip()\n",
    "    # Extracting RAM from the 1st card\n",
    "    ram = item.find('li',{'class':\"rgWa7D\"}).text[0:item.find('li',{'class':\"rgWa7D\"}).text.find('|')]\n",
    "    # Extracting Storage/ROM from 1st card\n",
    "    storage = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][0:10].strip()\n",
    "    # Extracting whether there is an option of expanding the storage or not\n",
    "    expandable = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][13:]\n",
    "    # Extracting the display option from the 1st card\n",
    "    display = item.find_all('li')[1].text.strip()\n",
    "    # Extracting camera options from the 1st card\n",
    "    camera = item.find_all('li')[2].text.strip()\n",
    "    # Extracting the battery option from the 1st card\n",
    "    battery = item.find_all('li')[3].text\n",
    "    # Extracting the processir option from the 1st card\n",
    "    processor = item.find_all('li')[4].text.strip()\n",
    "    # Extracting Warranty from the 1st card\n",
    "    warranty = item.find_all('li')[-1].text.strip()\n",
    "    # Extracting price of the model from the 1st card\n",
    "    price = item.find('div',{'class':'_30jeq3 _1_WHN1'}).text\n",
    "    result = (model,star,num_ratings,reviews,ram,storage,expandable,display,camera,battery,processor,warranty,price)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ceddda75",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-f7f428424766>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"_1fQZEK\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mrecords_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_phone_model_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-48-eecef856a2fc>\u001b[0m in \u001b[0;36mextract_phone_model_info\u001b[1;34m(item)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"_4rR01T\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Extracting Stars from 1st card\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mstar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"_3LWZlK\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Extracting Number of Ratings from 1st card\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mnum_ratings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"_2_R_DZ\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\xa0&\\xa0'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" ; \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"_2_R_DZ\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\xa0&\\xa0'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" ; \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# Now putting all the information from all the cards/phone models and putting them into a list\n",
    "records_list = []\n",
    "results = soup.find_all('a',{'class':\"_1fQZEK\"})\n",
    "for item in results:\n",
    "    records_list.append(extract_phone_model_info(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb03ec8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>star</th>\n",
       "      <th>num_ratings</th>\n",
       "      <th>reviews</th>\n",
       "      <th>ram</th>\n",
       "      <th>storage</th>\n",
       "      <th>expandable</th>\n",
       "      <th>display</th>\n",
       "      <th>camera</th>\n",
       "      <th>battery</th>\n",
       "      <th>processor</th>\n",
       "      <th>warranty</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model, star, num_ratings, reviews, ram, storage, expandable, display, camera, battery, processor, warranty, price]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(records_list,columns=['model',\"star\",\"num_ratings\"\n",
    "   ,\"reviews\",'ram',\"storage\",\"expandable\",\"display\",\"camera\",\"battery\",\"processor\",\"warranty\",\"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672adce1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
